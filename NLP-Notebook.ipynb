{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solid-alloy",
   "metadata": {},
   "source": [
    "# NLP - Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-album",
   "metadata": {},
   "source": [
    "In this notebook we will attempt to make a good clustering on our data set by using NLP (Natural Language Processing) methods. Since the data set does not only contain visual information, but also very specific titles of the products, it seems not to be unlikely that this text information can make a substantial contribution to the accuracy of our prediction.\n",
    "\n",
    "At the end of this notebook, the results we got from our NLP approach are combined with those from the pHash-Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-decline",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convinced-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subjective-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recent-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .csv file containing the text information to each image \n",
    "MY_PATH = './data'\n",
    "def load_trainCsv():\n",
    "    df = pd.read_csv(MY_PATH + \"/train.csv\")   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-eclipse",
   "metadata": {},
   "source": [
    "# Functions for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-parker",
   "metadata": {},
   "source": [
    "In this notebook, we will build two NLP models. One model is based on the TfidfVectorizer from sklearn and one model based on Word2Vec. In order to evaluate these models later on, we now define some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "christian-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score_i(cl_real_i, cl_pred_i):\n",
    "    '''\n",
    "    Description:\n",
    "    Calculate f-score for a single posting_id\n",
    "    f1-score is the mean of all f-scores\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (list): list of posting_id's belonging to the real cluster\n",
    "    argument2 (list): list of posting_id's belonging to the predicted cluster\n",
    "    \n",
    "    Returns: \n",
    "    float value of f-score   \n",
    "    '''\n",
    "    \n",
    "    s_pred = set(cl_pred_i)\n",
    "    s_real = set(cl_real_i)\n",
    "    s_intsec = s_pred.intersection(s_real)\n",
    "\n",
    "    return 2*len(s_intsec) / (len(s_pred)+len(s_real))\n",
    "\n",
    "\n",
    "def recall_i(cl_real_i, cl_pred_i):      \n",
    "    '''\n",
    "    Description:\n",
    "    Calculate recall for a single posting_id\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (list): list of posting_id's belonging to the real cluster\n",
    "    argument2 (list): list of posting_id's belonging to the predicted cluster\n",
    "    \n",
    "    Returns: \n",
    "    float value of recall   \n",
    "    '''\n",
    "        \n",
    "    s_pred = set(cl_pred_i)\n",
    "    s_real = set(cl_real_i)   \n",
    "    s_diff_r_p = s_real.difference(s_pred)\n",
    "    \n",
    "    return (len(s_real) - len(s_diff_r_p)) / len(s_real) \n",
    "\n",
    "\n",
    "def precision_i(cl_real_i, cl_pred_i):      \n",
    "    '''\n",
    "    Description:\n",
    "    Calculate precision for a single posting_id\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (list): list of posting_id's belonging to the real cluster\n",
    "    argument2 (list): list of posting_id's belonging to the predicted cluster\n",
    "    \n",
    "    Returns: \n",
    "    float value of precision   \n",
    "    '''\n",
    "    \n",
    "    s_pred = set(cl_pred_i)\n",
    "    s_real = set(cl_real_i)    \n",
    "    s_diff_p_r = s_pred.difference(s_real)\n",
    "    \n",
    "    return (len(s_pred) - len(s_diff_p_r)) / len(s_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "backed-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_cluster_of_i_w2v(i):\n",
    "    '''\n",
    "    Description:\n",
    "    Find real cluster for a single posting_id\n",
    "    Use this function when working with Word2Vec\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (int): position of posting_id in DataFrame\n",
    "    \n",
    "    Returns: \n",
    "    list of all posting_id's  \n",
    "    '''\n",
    "    \n",
    "    l_g = (df_train_w2v.iloc[i].at['label_group'])\n",
    "    df_red = df_train_w2v[df_train_w2v['label_group'] == l_g]\n",
    "    df_red_list = df_red['posting_id'].tolist()\n",
    "    return df_red_list\n",
    "\n",
    "\n",
    "def real_cluster_of_i_tfidf(i):\n",
    "    '''\n",
    "    Description:\n",
    "    Find real cluster for a single posting_id\n",
    "    Use this function when working with TfidVectorizer\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (int): position of posting_id in DataFrame\n",
    "    \n",
    "    Returns: \n",
    "    list of all posting_id's  \n",
    "    '''\n",
    "        \n",
    "    l_g = (df_train_tfidf.iloc[i].at['label_group'])\n",
    "    df_red = df_train_tfidf[df_train_tfidf['label_group'] == l_g]\n",
    "    df_red_list = df_red['posting_id'].tolist()\n",
    "    return df_red_list\n",
    "\n",
    "\n",
    "def pred_cluster_of_i_tfidf(i,threshold):\n",
    "    '''\n",
    "    Description:\n",
    "    Find predicted cluster for a single posting_id\n",
    "    Use this function when working with TfidVectorizer\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (int): position of posting_id in DataFrame\n",
    "    \n",
    "    Returns: \n",
    "    list of all posting_id's  \n",
    "    '''\n",
    "    \n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "    \n",
    "    for j in range(len(corpus)):\n",
    "        list1.append(round(dist2(i, j),3))\n",
    "        list2.append(labels_tfidf[j])\n",
    "        list3.append(posting_id_tfidf[j])\n",
    "        \n",
    "    df_nlp = pd.DataFrame(data = [list1,list2,list3]).transpose()    \n",
    "    df_nlp = df_nlp[df_nlp[0] <= threshold]    \n",
    "    ls = df_nlp[2].tolist()\n",
    "    \n",
    "    return ls\n",
    "\n",
    "\n",
    "def pred_cluster_of_i_w2v(i,threshold):\n",
    "    '''\n",
    "    Description:\n",
    "    Find predicted cluster for a single posting_id\n",
    "    Use this function when working with Word2Vec\n",
    "    \n",
    "    Parameters: \n",
    "    argument1 (int): position of posting_id in DataFrame\n",
    "    \n",
    "    Returns: \n",
    "    list of all posting_id's  \n",
    "    '''\n",
    "\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "        \n",
    "    for j in range(34250):\n",
    "\n",
    "        i_vec_1 = df_train_w2v['word_vec'][j]\n",
    "        i_vec_2 = df_train_w2v['word_vec'][i]\n",
    "        list1.append(round(get_sim_two_pi(i_vec_1, i_vec_2),4))\n",
    "        \n",
    "        list2.append(labels[j])\n",
    "        list3.append(posting_id[j])\n",
    "                \n",
    "    df_nlp = pd.DataFrame(data = [list1,list2,list3]).transpose()\n",
    "    df_nlp = df_nlp.sort_values(by = 0)\n",
    "    \n",
    "    df_nlp = df_nlp[df_nlp[0] >= threshold]\n",
    "    \n",
    "    ls = df_nlp[2].tolist()\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-nature",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-phoenix",
   "metadata": {},
   "source": [
    "## Creating a feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-insert",
   "metadata": {},
   "source": [
    "In order to compare to images, we will first transform each title into a feature vector. The distance of the respective images can then be measured by the distance between these two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "close-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean all the titles of our images\n",
    "def clean_title(title):\n",
    "    title = title.replace(\"-\", \" \").replace(\"//\", \" \").replace(\"[\", \" \").replace(\"]\", \" \").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "    title = title.replace(\"+\", \" \").replace(\"/\", \" \").replace(\"x\", \" \").replace(\"x\", \" \").replace(\"\\\\\", \" \")\n",
    "    title = title.replace(\",\", \" \")\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coastal-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tfidf = load_trainCsv()\n",
    "df_train_tfidf['cleanTitle'] = df_train_tfidf['title'].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arbitrary-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_train_tfidf['cleanTitle'].to_list()\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "st_words = nltk.corpus.stopwords.words(\"english\")\n",
    "st_words.extend(nltk.corpus.stopwords.words(\"indonesian\"))\n",
    "\n",
    "# Deleting all words which consist only of one or two letters\n",
    "for w in word_index.keys():\n",
    "    if len(w)<3:\n",
    "        st_words.append(w)\n",
    "    if w.isnumeric():\n",
    "        st_words.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-square",
   "metadata": {},
   "source": [
    "The hyperparameter max_features is very important. It determines the length of the feature vectors. The longer these vectors are, the more precise our predictions will be. On the other hand, this will make calculations slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floppy-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now vectorize all our string included in the corpus; use TfidfVectorizer from Sklearn\n",
    "vectorizer = TfidfVectorizer(stop_words=st_words, max_features=2500)   \n",
    "vectorizer.fit(corpus)\n",
    "label_vec = vectorizer.transform(corpus)\n",
    "\n",
    "label_vec = label_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-funds",
   "metadata": {},
   "source": [
    "### Defining a distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-visiting",
   "metadata": {},
   "source": [
    "There are theoretically infinite possibilities to define distance functions as we can use the Minkowski Metric with any natural number p. Here we just experimented with p=1 (the so called Manhattan Metric) and p=2 (The Euclidean Metric). It turned out that the Euclidean Metric gives us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cloudy-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist2(x,y):          # x,y indicate the position of the two images in our DataFrame\n",
    "    a = label_vec[x]\n",
    "    b = label_vec[y]\n",
    "    dist = np.sqrt(sum([(a[i] - b[i])**2 for i in range(label_vec.shape[1])]))        # (Euclidean Metric)\n",
    "    #dist = sum([abs((a[i] - b[i])) for i in range(label_vec.shape[1])])              # (Manhattan-Metric)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-straight",
   "metadata": {},
   "source": [
    "### Finding semantically similar pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-comfort",
   "metadata": {},
   "source": [
    "We now have a look at a particular picture and display the 15 images out of our data set which are semantically closest to this picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cheap-helmet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_598686012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14689</th>\n",
       "      <td>0.29</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_4187857506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>0.29</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_578257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>0.51</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_2710843880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22962</th>\n",
       "      <td>0.67</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_395732057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20890</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30737591</td>\n",
       "      <td>train_462541577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2077604114</td>\n",
       "      <td>train_4161009603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21009</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2545212005</td>\n",
       "      <td>train_1675502287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>1.0</td>\n",
       "      <td>179570104</td>\n",
       "      <td>train_3639374022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27513</th>\n",
       "      <td>1.0</td>\n",
       "      <td>599988605</td>\n",
       "      <td>train_3547727499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23521</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2227313314</td>\n",
       "      <td>train_528855712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23523</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2858826238</td>\n",
       "      <td>train_2162750166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1450032032</td>\n",
       "      <td>train_1468886633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3967663824</td>\n",
       "      <td>train_4193006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33333</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3723784552</td>\n",
       "      <td>train_2462114234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1                 2\n",
       "8400    0.0   912146474   train_598686012\n",
       "14689  0.29   912146474  train_4187857506\n",
       "17008  0.29   912146474   train_578257500\n",
       "3326   0.51   912146474  train_2710843880\n",
       "22962  0.67   912146474   train_395732057\n",
       "20890   1.0    30737591   train_462541577\n",
       "7262    1.0  2077604114  train_4161009603\n",
       "21009   1.0  2545212005  train_1675502287\n",
       "4097    1.0   179570104  train_3639374022\n",
       "27513   1.0   599988605  train_3547727499\n",
       "23521   1.0  2227313314   train_528855712\n",
       "23523   1.0  2858826238  train_2162750166\n",
       "994     1.0  1450032032  train_1468886633\n",
       "3946    1.0  3967663824  train_4193006014\n",
       "33333   1.0  3723784552  train_2462114234"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tfidf = df_train_tfidf['label_group'].to_list()\n",
    "posting_id_tfidf = df_train_tfidf['posting_id'].to_list()\n",
    "\n",
    "dist_ls = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    dist_ls.append(round(dist2(8400, i),2))\n",
    "\n",
    "df_nlp = pd.DataFrame(data = [dist_ls,labels_tfidf,posting_id_tfidf]).transpose()    \n",
    "\n",
    "df_nlp = df_nlp.sort_values(by = 0)\n",
    "df_nlp.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-payday",
   "metadata": {},
   "source": [
    "In column 0 we see the distance the respective image (8400) has to the image represented by the row number. In column 1 we see the cluster this image belongs to. We notice that the semantically nearest neighbours of image 8400 are images which actually belong to the same cluster. (Of course, our metric does not always perform that well.)\n",
    "\n",
    "One important hyperparameter we will have to tune is the threshold. This threshold determines how \"close\" another image has to be to the image we're just looking at in order to make the assumption that the two images display the same product. The table above indicates that the threshold could be at around 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-astronomy",
   "metadata": {},
   "source": [
    "### Estimating the F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-daniel",
   "metadata": {},
   "source": [
    "In order to estimate the F1-Score we can expect from that method, we now apply our f_score_i function to a certain number of images. The exact F1-Score is the mean of these accuracy values of all the 34250 images.\n",
    "\n",
    "What does the f_score_i function do: First construct the intersection of the two sets which represent the real cluster and predicted cluster. Then divide the length of this intersection through the sum of the length of both real and predicted cluster. The result is then multiplied by two and is finally returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "presidential-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tfidf = []\n",
    "\n",
    "for i in range(100):\n",
    "    clreal = real_cluster_of_i_tfidf(i*100)\n",
    "    clpred = pred_cluster_of_i_tfidf(i*100,0.7)\n",
    "    pr = f_score_i(clreal,clpred)\n",
    "    pred_tfidf.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "included-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5831381080605215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_tfidf)/len(pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-upset",
   "metadata": {},
   "source": [
    "### Excursion: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-skill",
   "metadata": {},
   "source": [
    "In order to decrease the calculation time, we now will do a dimensionality reduction via Principal Component Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coral-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_PCA(data, n_components=300):\n",
    "    p = PCA(n_components=n_components, random_state=42)\n",
    "    p.fit(data)    \n",
    "    return p\n",
    "\n",
    "feat_vec_pca = create_fit_PCA(label_vec)\n",
    "vec_pca = feat_vec_pca.transform(label_vec)\n",
    "\n",
    "def dist_pca(x,y):          # x,y indicate the position of the two images in our DataFrame\n",
    "    a = vec_pca[x]\n",
    "    b = vec_pca[y]\n",
    "    dist = np.sqrt(sum([(a[i] - b[i])**2 for i in range(100)]))           # p=2 (Euclidean Metric)\n",
    "    #dist = sum([abs((a[i] - b[i])) for i in range(label_vec.shape[1])])  # p=1 (Manhattan-Metric)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "engaging-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3697043299</td>\n",
       "      <td>train_804001417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>0.11</td>\n",
       "      <td>410540079</td>\n",
       "      <td>train_3958025844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>0.11</td>\n",
       "      <td>387772861</td>\n",
       "      <td>train_3913619870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.11</td>\n",
       "      <td>3697043299</td>\n",
       "      <td>train_1337940039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>0.11</td>\n",
       "      <td>1377712939</td>\n",
       "      <td>train_1496501644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31227</th>\n",
       "      <td>0.12</td>\n",
       "      <td>3697043299</td>\n",
       "      <td>train_2388554702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>0.12</td>\n",
       "      <td>410540079</td>\n",
       "      <td>train_625253343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>0.12</td>\n",
       "      <td>961662899</td>\n",
       "      <td>train_1414655801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13127</th>\n",
       "      <td>0.13</td>\n",
       "      <td>1460273711</td>\n",
       "      <td>train_2153757835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24479</th>\n",
       "      <td>0.14</td>\n",
       "      <td>4138426580</td>\n",
       "      <td>train_1312282384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16850</th>\n",
       "      <td>0.14</td>\n",
       "      <td>1469816250</td>\n",
       "      <td>train_1925689397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15351</th>\n",
       "      <td>0.14</td>\n",
       "      <td>1639324861</td>\n",
       "      <td>train_614699818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20043</th>\n",
       "      <td>0.14</td>\n",
       "      <td>4138426580</td>\n",
       "      <td>train_1725639998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16436</th>\n",
       "      <td>0.14</td>\n",
       "      <td>1939849132</td>\n",
       "      <td>train_3036907789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25991</th>\n",
       "      <td>0.14</td>\n",
       "      <td>1639324861</td>\n",
       "      <td>train_2507672437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1                 2\n",
       "1100    0.0  3697043299   train_804001417\n",
       "6175   0.11   410540079  train_3958025844\n",
       "2513   0.11   387772861  train_3913619870\n",
       "973    0.11  3697043299  train_1337940039\n",
       "7601   0.11  1377712939  train_1496501644\n",
       "31227  0.12  3697043299  train_2388554702\n",
       "12453  0.12   410540079   train_625253343\n",
       "9916   0.12   961662899  train_1414655801\n",
       "13127  0.13  1460273711  train_2153757835\n",
       "24479  0.14  4138426580  train_1312282384\n",
       "16850  0.14  1469816250  train_1925689397\n",
       "15351  0.14  1639324861   train_614699818\n",
       "20043  0.14  4138426580  train_1725639998\n",
       "16436  0.14  1939849132  train_3036907789\n",
       "25991  0.14  1639324861  train_2507672437"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_ls_pca = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    dist_ls_pca.append(round(dist_pca(1100, i),2))\n",
    "\n",
    "df_nlp = pd.DataFrame(data = [dist_ls_pca,labels_tfidf,posting_id_tfidf]).transpose()    \n",
    "\n",
    "df_nlp = df_nlp.sort_values(by = 0)\n",
    "df_nlp.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-harmony",
   "metadata": {},
   "source": [
    "We see that calculation time decreases a lot. Looking at some examples, we can see that the results we get are not very reliable. Future work on this issue could include checking out precisely the benefits and the limits of the PCA method for the NLP approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-fault",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-needle",
   "metadata": {},
   "source": [
    "As the TfidfVectorizer need quite a lot of computation time, we now try out a second NLP method: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-renaissance",
   "metadata": {},
   "source": [
    "### Preprocessing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alert-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friendly borrowed by Nikhil Manali\n",
    "# https://www.kaggle.com/coder247/similarity-using-word2vec-text\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            if token == 'xxxx':\n",
    "                continue\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def word2vec_model(size_feat_vec=50):\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                     window=3,\n",
    "                     vector_size=size_feat_vec,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)\n",
    "    \n",
    "    w2v_model.build_vocab(processed_docs)\n",
    "    w2v_model.train(processed_docs, total_examples=w2v_model.corpus_count, epochs=300, report_delay=1)\n",
    "    \n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spare-prayer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>preprocess_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[paper, victoria, secret]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>[doubl, tape, origin, doubl, foam, tape]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                          preprocess_title\n",
       "0   train_129225211                 [paper, victoria, secret]\n",
       "1  train_3386243561  [doubl, tape, origin, doubl, foam, tape]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_w2v = load_trainCsv()\n",
    "processed_docs = df_train_w2v['title'].map(preprocess)\n",
    "processed_docs = list(processed_docs)\n",
    "\n",
    "df_train_w2v['preprocess_title']=processed_docs\n",
    "df_train_w2v[['posting_id','preprocess_title']][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-wednesday",
   "metadata": {},
   "source": [
    "### Building a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secondary-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_new_model_bool = False\n",
    "\n",
    "if build_new_model_bool:\n",
    "    w2v_model = word2vec_model()\n",
    "    w2v_model.save('word2vec_model')\n",
    "\n",
    "else:\n",
    "    w2v_model = pickle.load(open('word2vec_model', 'rb')) \n",
    "    \n",
    "emb_vec = w2v_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-tulsa",
   "metadata": {},
   "source": [
    "### Create a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expired-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>preprocess_title</th>\n",
       "      <th>word_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[paper, victoria, secret]</td>\n",
       "      <td>[-0.04777907373336296, 0.16484181570560288, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[doubl, tape, origin, doubl, foam, tape]</td>\n",
       "      <td>[0.14724423593913194, 0.1192708373649622, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "\n",
       "                           preprocess_title  \\\n",
       "0                 [paper, victoria, secret]   \n",
       "1  [doubl, tape, origin, doubl, foam, tape]   \n",
       "\n",
       "                                            word_vec  \n",
       "0  [-0.04777907373336296, 0.16484181570560288, 0....  \n",
       "1  [0.14724423593913194, 0.1192708373649622, -0.0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature_vec_v2(sen1, model, size_feat_vec=50):\n",
    "    \n",
    "    sen_vec1 = np.zeros(size_feat_vec)\n",
    "    for val in sen1:\n",
    "        sen_vec1 = np.add(sen_vec1, model[val])    \n",
    "    return sen_vec1/norm(sen_vec1)\n",
    "\n",
    "df_train_w2v['word_vec'] = df_train_w2v.apply(\n",
    "                        lambda row: \n",
    "                        get_feature_vec_v2(row['preprocess_title'], emb_vec), \n",
    "                        axis=1)\n",
    "\n",
    "df_train_w2v.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-samuel",
   "metadata": {},
   "source": [
    "### Calculate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dirty-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_all_pi(i_vec_1,i_vec_all):  \n",
    "    return i_vec_all.dot(i_vec_1)\n",
    "\n",
    "\n",
    "def get_sim_two_pi(i_vec_1,i_vec_2):\n",
    "    sim = dot(i_vec_1,i_vec_2)/(norm(i_vec_1)*norm(i_vec_2))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "frank-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>1.0</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_598686012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>0.986869</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_2710843880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14689</th>\n",
       "      <td>0.985473</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_4187857506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>0.981489</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_578257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22962</th>\n",
       "      <td>0.908996</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_395732057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29207</th>\n",
       "      <td>0.701713</td>\n",
       "      <td>1813765221</td>\n",
       "      <td>train_3947026216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20472</th>\n",
       "      <td>0.695985</td>\n",
       "      <td>3195941759</td>\n",
       "      <td>train_1751507239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10514</th>\n",
       "      <td>0.692491</td>\n",
       "      <td>1813765221</td>\n",
       "      <td>train_722294539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>0.663119</td>\n",
       "      <td>342597810</td>\n",
       "      <td>train_599753440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16961</th>\n",
       "      <td>0.659976</td>\n",
       "      <td>1776453982</td>\n",
       "      <td>train_3522099472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1                 2\n",
       "8400        1.0   912146474   train_598686012\n",
       "3326   0.986869   912146474  train_2710843880\n",
       "14689  0.985473   912146474  train_4187857506\n",
       "17008  0.981489   912146474   train_578257500\n",
       "22962  0.908996   912146474   train_395732057\n",
       "29207  0.701713  1813765221  train_3947026216\n",
       "20472  0.695985  3195941759  train_1751507239\n",
       "10514  0.692491  1813765221   train_722294539\n",
       "12462  0.663119   342597810   train_599753440\n",
       "16961  0.659976  1776453982  train_3522099472"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding semantically similar images, same as above\n",
    "\n",
    "id_1=8400\n",
    "i_vec_1 = df_train_w2v['word_vec'][id_1]\n",
    "i_vec_all = df_train_w2v['word_vec'].values\n",
    "i_vec_all = np.vstack(i_vec_all)\n",
    "\n",
    "labels = df_train_w2v['label_group'].to_list()\n",
    "posting_id = df_train_w2v['posting_id'].to_list()\n",
    "list1 = list(get_sim_all_pi(i_vec_1,i_vec_all))\n",
    "\n",
    "df_nlp = pd.DataFrame(data = [list1,labels,posting_id]).transpose()\n",
    "df_nlp = df_nlp.sort_values(by = [0,1],ascending=False)\n",
    "df_nlp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-expression",
   "metadata": {},
   "source": [
    "### Create clusters optimized on recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sixth-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall:  0.9343347338935574   Mean Length of cluster:  177.13\n"
     ]
    }
   ],
   "source": [
    "rec_values = []\n",
    "cl_size = []\n",
    "\n",
    "for i in range(100):\n",
    "    clreal = real_cluster_of_i_w2v(i*100)\n",
    "    clpred = pred_cluster_of_i_w2v(i*100,0.7)\n",
    "    rec_values.append(recall_i(clreal,clpred))\n",
    "    cl_size.append(len(clpred))\n",
    "    \n",
    "print(\"Mean Recall: \",sum(rec_values)/len(rec_values), \"  Mean Length of cluster: \", sum(cl_size)/len(cl_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-sellers",
   "metadata": {},
   "source": [
    "By building clusters using a threshold of 0.7 we get a mean cluster size of around 180. On the other hand, we see that the mean recall value is close to 1.  \n",
    "\n",
    "This is one of the most important findings of this notebook. That way we can reduce significantly the amount of images which possibly belong to a certain cluster: When looking for the cluster a certain image belongs to, instead of having to compare it to 34249 images, we now just need to look at 180, e.g. with visual methods. That way, the NLP method can be combined with other methods later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-consultancy",
   "metadata": {},
   "source": [
    "## Save results and combine them with pHash "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-emerald",
   "metadata": {},
   "source": [
    "Now we want to predict clusters for all of the 34250 images using a high threshold (0.97). This way we'll obtain clusters which are optimized on precision. The results will be saved in a dictionary and then combined with the results obtained in the pHash Notebook. This combination leads to a F1-Score of 66 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "specified-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_done = True\n",
    "\n",
    "if already_done == False:\n",
    "\n",
    "    dict_nlp_prec_all_97 = {}\n",
    "    list_post_id = df_train_w2v['posting_id'].tolist()\n",
    "\n",
    "    for i in range(34250):    \n",
    "        dict_nlp_prec_all_97[list_post_id[i]] = set(pred_cluster_of_i_w2v(i,0.97))\n",
    "        if i%1000 == 0:    # Display progress and save \n",
    "            print(i)\n",
    "            pickle.dump(dict_nlp_prec_all_97, open( \"dict_nlp_prec_all_97.p\", \"wb\" ) )\n",
    "    pickle.dump(dict_nlp_prec_all_97, open( \"dict_nlp_prec_all_97.p\", \"wb\" ) )    # Final save\n",
    "\n",
    "# Load results\n",
    "dict_nlp_prec_all_97_load = pickle.load( open( \"dict_nlp_prec_all_97.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "august-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two predictions\n",
    "def combi_pred(pred1, pred2):\n",
    "\n",
    "    combi_set = {} \n",
    "    for i in pred1.keys():\n",
    "        assert i in set(pred2.keys())\n",
    "        combi_set[i] = pred1[i].union(pred2[i])\n",
    "    return combi_set\n",
    "\n",
    "# Load results from the pHash Notebook\n",
    "dict_phash_prec_all_9_load = pickle.load( open( \"dict_phash_prec_all_9.p\", \"rb\" ) )\n",
    "\n",
    "pred_nlp_phash = combi_pred(dict_nlp_prec_all_97_load, dict_phash_prec_all_9_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "black-congo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.6647914689708506\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1-Score of the combination NLP and pHash\n",
    "\n",
    "fscores = []\n",
    "for i in range(34250):\n",
    "    x = f_score_i(real_cluster_of_i_w2v(i), pred_nlp_phash[df_train_w2v.posting_id.values[i]])\n",
    "    fscores.append(x)\n",
    "    \n",
    "print(\"F1-Score: \", sum(fscores)/len(fscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-onion",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-digest",
   "metadata": {},
   "source": [
    "- Most important: Saving (via pickle) clusters omtimized on recall and combine them with visual methods based on CNNs\n",
    "- Experimenting more on the hyperparameters of the W2V model\n",
    "- Use further NLP models based on the transformer framework (such as BERT) \n",
    "- Find out if PCA can be applied in order to decrease computation time but without loosing to much accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
