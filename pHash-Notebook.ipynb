{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civil-morning",
   "metadata": {},
   "source": [
    "# pHash - Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-literacy",
   "metadata": {},
   "source": [
    "In this notebook, we use the pHashes of the products contained in the csv-file. We define a distance function in order to measure the similarity of two images. At the end of the notebook, we build clusters optimized on precision. In the NLP notebook, these prediction will be combined with the results from the NLP method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-accreditation",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artistic-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imagehash\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disturbed-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PATH = './data'  \n",
    "\n",
    "df_train_all = pd.read_csv(MY_PATH + '/shopee-product-matching/train.csv')\n",
    "train_images = MY_PATH + '/shopee-product-matching/train_images' + '/' + df_train_all['image']\n",
    "df_train_all['path'] = train_images\n",
    "\n",
    "dic_label_group_posting_id = df_train_all.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "df_train_all['target'] = df_train_all.label_group.map(dic_label_group_posting_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-salem",
   "metadata": {},
   "source": [
    "## Create feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "miniature-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['phash_hex_to_hash'] = df_train_all['image_phash'].apply(lambda x: imagehash.hex_to_hash(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-valve",
   "metadata": {},
   "source": [
    "## Create a distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exact-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phash_distance(p_hash_vec_1,p_hash_vec_2):\n",
    "    '''\n",
    "    input:  p_hash_vec_1: one pHash vector\n",
    "            p_hash_vec_2: one or more pHash vectors\n",
    "    output: distance '''\n",
    "    return p_hash_vec_2 - p_hash_vec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-honduras",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1    42\n",
       "2    40\n",
       "3    30\n",
       "Name: phash_hex_to_hash, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "get_phash_distance(df_train_all['phash_hex_to_hash'][0],df_train_all['phash_hex_to_hash'][0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-state",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-lesson",
   "metadata": {},
   "source": [
    "Define some functions we'll need to do the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "improved-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score_i(i, threshold, \n",
    "              feature_vec_all = df_train_all['phash_hex_to_hash'],              \n",
    "              posting_id_ls   = df_train_all['posting_id'].to_list()):\n",
    "    feature_vec_i = feature_vec_all[i]\n",
    "    # predicted cluster \n",
    "    s_pred            =  set(pred_cluster_of_i(threshold, feature_vec_i, feature_vec_all, posting_id_ls ))\n",
    "    # real cluster \n",
    "    s_real            =  set(df_train_all['target'][i])\n",
    "    # intersection of real- and predicted cluster\n",
    "    int_sec_pred_real = s_pred.intersection(s_real)\n",
    "    return (2 * len(int_sec_pred_real)/(len(s_pred)+len(s_real))), s_pred, len(s_pred), s_real \n",
    "\n",
    "\n",
    "def recall_i(i, threshold,\n",
    "              feature_vec_all = df_train_all['phash_hex_to_hash'],              \n",
    "              posting_id_ls   = df_train_all['posting_id'].to_list()): \n",
    "    \n",
    "    feature_vec_i = feature_vec_all[i]\n",
    "    s_pred = set(pred_cluster_of_i(threshold, feature_vec_i, feature_vec_all, posting_id_ls ))\n",
    "    s_real = set(df_train_all['target'][i])\n",
    "    c = s_real.difference(s_pred)\n",
    "    return (len(s_real) - len(c)) / len(s_real), s_pred ,len(s_pred), s_real\n",
    "\n",
    "def precision_i(i, threshold,  \n",
    "              feature_vec_all = df_train_all['phash_hex_to_hash'],              \n",
    "              posting_id_ls   = df_train_all['posting_id'].to_list()):  \n",
    "    \n",
    "    feature_vec_i = feature_vec_all[i]\n",
    "    s_pred = set(pred_cluster_of_i(threshold, feature_vec_i, feature_vec_all, posting_id_ls ))\n",
    "    s_real = set(df_train_all['target'][i])\n",
    "    return ((len(s_pred) - len(s_pred.difference(s_real))) / len(s_pred)), s_pred, len(s_pred), s_real\n",
    "\n",
    "def pred_cluster_of_i(threshold, feature_vec_i,\n",
    "                      feature_vec_all = df_train_all['phash_hex_to_hash'],                      \n",
    "                      posting_id_ls   = df_train_all['posting_id'].to_list()):\n",
    "    \n",
    "    p_hash_i   = feature_vec_i\n",
    "    p_hash_all = feature_vec_all\n",
    "    diff_1     = get_phash_distance(p_hash_i,p_hash_all)\n",
    "    list1      = list(diff_1)\n",
    "\n",
    "        \n",
    "    df_diff = pd.DataFrame(data = [list1,posting_id_ls]).transpose()    \n",
    "    df_diff = df_diff[df_diff[0] <= threshold]\n",
    "    \n",
    "    ls = df_diff[1].tolist()\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-prize",
   "metadata": {},
   "source": [
    "### Prediction with threshold = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complicated-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated precision:    0.963087748833363\n",
      "Estimated F1-Score:    0.5874131213621182\n"
     ]
    }
   ],
   "source": [
    "fsc_9 = []\n",
    "prec_9 = []\n",
    "for i in range(0,342):\n",
    "    fsc_9.append(f_score_i(i*100,9)[0])\n",
    "    prec_9.append(precision_i(i*100,9)[0])\n",
    "\n",
    "print(\"Estimated precision:   \", sum(prec_9)/len(prec_9))\n",
    "print(\"Estimated F1-Score:   \", sum(fsc_9)/len(fsc_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-custom",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-advance",
   "metadata": {},
   "source": [
    "Now we make a prediction optimized on precision and save the results with pickle. These will be used at the end of the NLP-Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "western-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_done = True\n",
    "\n",
    "if already_done == False:\n",
    "\n",
    "    dict_prec_all_9 = {}\n",
    "    for i in range(0,34250):\n",
    "        dict_prec_all_9[i] = precision_i(i, threshold=9)[0:2]\n",
    "\n",
    "        if i % 3000 == 0:\n",
    "            # Save\n",
    "            pickle.dump(dict_prec_all_9, open( \"dict_prec_all_9.p\", \"wb\" ) )\n",
    "            print(i)\n",
    "\n",
    "    # FinalSave\n",
    "    pickle.dump(dict_prec_all_9, open( \"dict_prec_all_9.p\", \"wb\" ) )\n",
    "    \n",
    "    # Load\n",
    "    dict_prec_all_9_load = pickle.load( open( \"dict_prec_all_9.p\", \"rb\" ) )\n",
    "    \n",
    "    # Change keys of dictionary\n",
    "    list_post_id = df_train_all['posting_id'].tolist()\n",
    "\n",
    "    dict_phash_prec_all_9 = {}\n",
    "    for i in range(34250):\n",
    "        dict_phash_prec_all_9[list_post_id[i]] = dict_prec_all_9_load[i][1]\n",
    "    pickle.dump(dict_phash_prec_all_9, open( \"dict_phash_prec_all_9.p\", \"wb\" ) )\n",
    "\n",
    "# Load  \n",
    "dict_phash_prec_all_9_load = pickle.load( open( \"dict_phash_prec_all_9.p\", \"rb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
