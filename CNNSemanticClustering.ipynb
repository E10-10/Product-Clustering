{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a943ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style='color:white; background:orange; border:0'>CNN Semantic Clustering - by Kay Delventhal</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565715f",
   "metadata": {},
   "source": [
    "### Capstone Project: Product Clustering\n",
    "\n",
    "Coauthors: Elias BÃ¼chner, Kay Delventhal, Niels-Christian Leight and Phillip McRae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae414eaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style='color:white; background:orange; border:0'>Classify images and collect labels to generate semantic features.</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c99998",
   "metadata": {},
   "source": [
    "This Notebook is based on Python 3.6 - for the benefit of GPU processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698d3fe",
   "metadata": {},
   "source": [
    "### The Idea:\n",
    "\n",
    "As an alternative to technical based algorithms like pHash or ColorMatch, which use number-based features, semantic features would allow to cluster images not just based on similar images - but by embedded semantic meaning.\n",
    "\n",
    "A number of pre-trained CNNs were used, to classifiy all 32412 Shoppee images. All CNNs used were pre-traind on the \"ImageNet\" data-set which is based on 1000 classes. Therefore they all use the same classes for prediction.\n",
    "\n",
    "The goal is **not** to obtain a single valid classification, but rather to obtain an **array of labels** with rated probabilities. This **array of labels** generated by different CNNs, gives us the possibility to count appearances of classes. \n",
    "\n",
    "This counting is done based on different metrics:\n",
    "- N bests of all CNNs\n",
    "- N better then 'threshold'\n",
    "- Misc\n",
    "\n",
    "After a selection is made, a feature generating process is used to create new semantic based features per image. These new semantic based features can be used with a NLP algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b67ab",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "For this notebook the following data is needed: https://www.kaggle.com/c/shopee-product-matching/overview/evaluation\n",
    "- shopee-product-matching/train_images\n",
    "- shopee-product-matching/test_images\n",
    "- train.csv\n",
    "- test.csv\n",
    "- sample_submission.csv\n",
    "\n",
    "For this notebook the following python files are needed:\n",
    "- SemanticClustering.py\n",
    "- CSData.py\n",
    "- CSScoring.py\n",
    "- CSTools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802bf68",
   "metadata": {},
   "source": [
    "### Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359c2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python modul import\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "import importlib # to reload modules\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f85dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# basic CNN tools modul import\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1356b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modul import for for CNN grid-search\n",
    "import keras.applications.xception as xcptn\n",
    "import keras.applications.vgg16 as vgg16\n",
    "import keras.applications.vgg19 as vgg19\n",
    "import keras.applications.densenet as denet\n",
    "import keras.applications.resnet50 as rsnt50\n",
    "import keras.applications.resnet as resnet\n",
    "import keras.applications.nasnet as nasnet\n",
    "import keras.applications.mobilenet_v2 as mblntv2\n",
    "import keras.applications.inception_v3 as incptv3\n",
    "import keras.applications.inception_resnet_v2 as incntv2\n",
    "#import keras.applications.efficientnet as efcntnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2408ea96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used for NLP setup\n",
    "import random\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#[nltk_data] Downloading package wordnet to\n",
    "#[nltk_data]     C:\\Users\\vfx\\AppData\\Roaming\\nltk_data...\n",
    "#[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a97789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tools to solve GPU issues\n",
    "import gc\n",
    "gc.collect()\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257d9c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Project Tools and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f37a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/_DataScience/CapstoneProject/\n",
      "D:/_DataScience/CapstoneProject/\n",
      "D:/_DataScience/CapstoneProject/CnnGridSearch/\n"
     ]
    }
   ],
   "source": [
    "# python module import - cgn-data-21-1 Capstone Project tools\n",
    "import os\n",
    "import importlib # to reload modules\n",
    "\n",
    "import CSTools as ct\n",
    "importlib.reload(ct)\n",
    "\n",
    "import CSData as cd\n",
    "importlib.reload(cd)\n",
    "\n",
    "import CSScoring as sc\n",
    "importlib.reload(sc)\n",
    "\n",
    "import SemanticClustering as sem\n",
    "importlib.reload(sem)\n",
    "\n",
    "# base path setup\n",
    "PATH = os.getcwd().replace('\\\\','/')+'/'\n",
    "print(PATH)\n",
    "DATA = PATH+'CnnGridSearch'+'/'\n",
    "if not os.path.isdir(DATA):\n",
    "    os.makedirs(DATA)\n",
    "print(PATH)\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf896d0",
   "metadata": {},
   "source": [
    "## Define CNN Grid-Search Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02fddc",
   "metadata": {},
   "source": [
    "Pre-trained **SciKit-learn** models are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c83ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global vars for image and label data\n",
    "SHOPEEIMG_TRAIN = PATH+'shopee-product-matching/train_images'\n",
    "SHOPEEDATA_TRAIN = PATH+'shopee-product-matching/train.csv'\n",
    "\n",
    "# define CNN grid-search dict()\n",
    "# to be used with classify_all()\n",
    "models = {\n",
    "    'Xception':    {'CNN': xcptn.Xception, 'PREP': xcptn.preprocess_input, 'DECO': xcptn.decode_predictions, 'SIZE': (299, 299)},\n",
    "    'VGG16':       {'CNN': vgg16.VGG16, 'PREP': vgg16.preprocess_input, 'DECO': vgg16.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'VGG19':       {'CNN': vgg19.VGG19, 'PREP': vgg19.preprocess_input, 'DECO': vgg19.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'DenseNet201': {'CNN': denet.DenseNet201, 'PREP': denet.preprocess_input, 'DECO': denet.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'NASNetLarge': {'CNN': nasnet.NASNetLarge, 'PREP': nasnet.preprocess_input, 'DECO': nasnet.decode_predictions, 'SIZE': (331, 331)},\n",
    "    'MobileNetV2': {'CNN': mblntv2.MobileNetV2, 'PREP': mblntv2.preprocess_input, 'DECO': mblntv2.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'ResNet50':    {'CNN': rsnt50.ResNet50, 'PREP': rsnt50.preprocess_input, 'DECO': rsnt50.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'ResNet152':   {'CNN': resnet.ResNet152, 'PREP': resnet.preprocess_input, 'DECO': resnet.decode_predictions, 'SIZE': (224, 224)},\n",
    "    'InceptionV3': {'CNN': incptv3.InceptionV3, 'PREP': incptv3.preprocess_input, 'DECO': incptv3.decode_predictions, 'SIZE': (299, 299)},\n",
    "    'IncResNetV2': {'CNN': incntv2.InceptionResNetV2, 'PREP': incntv2.preprocess_input, 'DECO': incntv2.decode_predictions, 'SIZE': (299, 299)}#,\n",
    "    #'EfficNetB7':  {'CNN': efcntnt.EfficientNetB7, 'PREP': efcntnt.preprocess_input, 'DECO': efcntnt.decode_predictions} # not included in python 3.6 keras version\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576670d",
   "metadata": {},
   "source": [
    "### Find Images for CNN Grid-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34826128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 32412\n",
      "cnn #: 10\n",
      "dict_keys(['Xception', 'VGG16', 'VGG19', 'DenseNet201', 'NASNetLarge', 'MobileNetV2', 'ResNet50', 'ResNet152', 'InceptionV3', 'IncResNetV2'])\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gather images for classify grid-sreach with classify_all()\n",
    "\n",
    "images = [x.replace('\\\\','/') for x in glob(SHOPEEIMG_TRAIN+'/*.jpg')]\n",
    "print('images:', len(images))\n",
    "\n",
    "use = models.keys()\n",
    "print('cnn #:', len(use))\n",
    "print(use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5a7d4",
   "metadata": {},
   "source": [
    "### CNN Gid-Search (Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fad634",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# classify grid-search\n",
    "\n",
    "print('images:', len(images))\n",
    "collect = sem.classify_all(images,models,top=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f8d42aa",
   "metadata": {},
   "source": [
    "# run from 09.04.2021\n",
    "images: 32412\n",
    "1 of 10 model: Xception\n",
    "-> time: 13.99 m\n",
    "2 of 10 model: VGG16\n",
    "-> time: 7.32 m\n",
    "3 of 10 model: VGG19\n",
    "-> time: 8.14 m\n",
    "4 of 10 model: DenseNet201\n",
    "-> time: 37.51 m\n",
    "5 of 10 model: NASNetLarge\n",
    "-> time: 50.43 m\n",
    "6 of 10 model: MobileNetV2\n",
    "-> time: 11.58 m\n",
    "7 of 10 model: ResNet50\n",
    "-> time: 12.83 m\n",
    "8 of 10 model: ResNet152\n",
    "-> time: 29.63 m\n",
    "9 of 10 model: InceptionV3\n",
    "-> time: 17.51 m\n",
    "10 of 10 model: IncResNetV2\n",
    "-> time: 36.27 m\n",
    "total time: 225.20 m\n",
    "total time: 3.75 h\n",
    "write: D:/_DataScience/CapstoneProject/collect.pickle\n",
    "count 324120\n",
    "Wall time: 3h 45min 21s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db3685",
   "metadata": {},
   "source": [
    "### Save/Load CNN Grid-search Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0743c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read: D:/_DataScience/CapstoneProject/CnnGridSearch/collect.pickle\n",
      "32412\n"
     ]
    }
   ],
   "source": [
    "#ct.write_dict('collect',collect,DATA)\n",
    "collect = ct.read_dict('collect',DATA)\n",
    "print(len(collect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c50ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process CNN Gird-Search Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f1325",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Functions to Retrive Data from Raw Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c722e",
   "metadata": {},
   "source": [
    "### Execute Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64ab064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 32412/32412 [00:09<00:00, 3340.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use apply_threshold() to gather classifications with probability over 'threshold' per CNN\n",
    "\n",
    "processing = dict()\n",
    "joind, found, count = sem.apply_threshold(collect, threshold = 0.1, show=0)\n",
    "processing['apply'] = joind, found, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba2cdec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 32412/32412 [00:02<00:00, 11678.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use find_predictions() to gather best three classifications guesses per CNN\n",
    "\n",
    "joind, found, count = sem.find_predictions(collect, get=3, show=0)\n",
    "processing['find'] = joind, found, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34bdd4",
   "metadata": {},
   "source": [
    "### Save/Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a4eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write: D:/_DataScience/CapstoneProject/CnnGridSearch/processing.pickle\n",
      "read: D:/_DataScience/CapstoneProject/CnnGridSearch/processing.pickle\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ct.write_dict('processing',processing,DATA)\n",
    "processing = ct.read_dict('processing',DATA)\n",
    "print(len(processing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee5f61",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Generate New Features from CNN Grid-Search Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67b8a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Find label frequency data per image across all CNNs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e96a170",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "\n",
    "for score in np.linspace(0.2,0.05,num=10):\n",
    "    print('score',score)\n",
    "    joind, found, count = sem.apply_threshold(collect, threshold=score, show=0)\n",
    "    all_tags, file_tags, tags_list, count_num, count_cnn = sem.tag_grouping(joind, booster=False, show=0)\n",
    "    print('0:',count_num[0])\n",
    "    print('2-6 :',sum([count_num[x] for x in range(2,6+1)]))\n",
    "    print('5-10:',sum([count_num[x] for x in range(5,len(models.keys())+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "572e8ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 32412/32412 [00:02<00:00, 11960.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0000a68812bc7e98c42888dfb1c07da0.jpg 2\n",
      "                                     LABEL ['confectionery*pillow*sock*wooden_spoon*wool', 'book_jacket*brassiere*comic_book*pillow*wool', 'brassiere*head_cabbage*shower_cap*wig*wool', 'brassiere*face_powder*handkerchief*rubber_eraser*wool', 'artichoke*confectionery*perfume*plastic_bag*wool', 'comic_book*jersey*maillot*pajama*pillow', 'book_jacket*comic_book*pajama*sock*sweatshirt', 'book_jacket*comic_book*pillow*sock*wool', 'bakery*comic_book*cowboy_hat*wooden_spoon*wool', 'artichoke*bakery*book_jacket*comic_book*wool']\n",
      "                                     TAGCK ['confectionery', 'pillow', 'sock', 'wooden_spoon', 'wool', 'book_jacket', 'brassiere', 'comic_book', 'pillow', 'wool', 'brassiere', 'head_cabbage', 'shower_cap', 'wig', 'wool', 'brassiere', 'face_powder', 'handkerchief', 'rubber_eraser', 'wool', 'artichoke', 'confectionery', 'perfume', 'plastic_bag', 'wool', 'comic_book', 'jersey', 'maillot', 'pajama', 'pillow', 'book_jacket', 'comic_book', 'pajama', 'sock', 'sweatshirt', 'book_jacket', 'comic_book', 'pillow', 'sock', 'wool', 'bakery', 'comic_book', 'cowboy_hat', 'wooden_spoon', 'wool', 'artichoke', 'bakery', 'book_jacket', 'comic_book', 'wool']\n",
      "                                     FOUND 8 wool ['Xception', 'VGG16', 'VGG19', 'DenseNet201', 'NASNetLarge', 'ResNet152', 'InceptionV3', 'IncResNetV2']\n",
      "                                     FOUND 6 comic_book ['VGG16', 'MobileNetV2', 'ResNet50', 'ResNet152', 'InceptionV3', 'IncResNetV2']\n",
      "                                     FOUND 4 pillow ['Xception', 'VGG16', 'MobileNetV2', 'ResNet152']\n",
      "                                     FOUND 4 book_jacket ['VGG16', 'ResNet50', 'ResNet152', 'IncResNetV2']\n",
      "                                     FOUND 3 sock ['Xception', 'ResNet50', 'ResNet152']\n",
      "                                     FOUND 3 brassiere ['VGG16', 'VGG19', 'DenseNet201']\n",
      "                                     FOUND 2 confectionery ['Xception', 'NASNetLarge']\n",
      "                                     FOUND 2 wooden_spoon ['Xception', 'InceptionV3']\n",
      "                                     FOUND 2 artichoke ['NASNetLarge', 'IncResNetV2']\n",
      "                                     FOUND 2 pajama ['MobileNetV2', 'ResNet50']\n",
      "                                     FOUND 2 bakery ['InceptionV3', 'IncResNetV2']\n",
      "                                     REST 8 38 ['head_cabbage', 'shower_cap', 'wig', 'face_powder', 'handkerchief', 'rubber_eraser', 'perfume', 'plastic_bag', 'jersey', 'maillot', 'sweatshirt', 'cowboy_hat']\n",
      "\n",
      "00039780dfc94d01db8676fe789ecd05.jpg 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|ââ                                                                          | 733/32412 [00:00<00:08, 3643.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "collect   32412\n",
      "joind     32412\n",
      "file_tags 32412\n",
      "\n",
      "143939 Xception\n",
      "138194 VGG16\n",
      "138894 VGG19\n",
      "140296 DenseNet201\n",
      "138125 NASNetLarge\n",
      "129329 MobileNetV2\n",
      "138902 ResNet50\n",
      "139203 ResNet152\n",
      "135120 InceptionV3\n",
      "142924 IncResNetV2\n",
      "\n",
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 4\n",
      "4 156\n",
      "5 605\n",
      "6 1380\n",
      "7 2554\n",
      "8 3928\n",
      "9 5594\n",
      "10 18191\n",
      "\n",
      "0: 0\n",
      "2-5 : 765\n",
      "5-10: 32252\n",
      "2-10: 32412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 32412/32412 [00:10<00:00, 3216.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0000a68812bc7e98c42888dfb1c07da0.jpg 2\n",
      "                                     LABEL ['pillow', 'book_jacket*pillow*wool', 'brassiere*head_cabbage*wool', 'face_powder', 'wool', 'jersey*pajama', 'comic_book*pajama*sock', 'book_jacket*comic_book', 'cowboy_hat*wool', 'artichoke']\n",
      "                                     TAGCK ['pillow', 'book_jacket', 'pillow', 'wool', 'brassiere', 'head_cabbage', 'wool', 'face_powder', 'wool', 'jersey', 'pajama', 'comic_book', 'pajama', 'sock', 'book_jacket', 'comic_book', 'cowboy_hat', 'wool', 'artichoke']\n",
      "                                     FOUND 4 wool ['VGG16', 'VGG19', 'NASNetLarge', 'InceptionV3']\n",
      "                                     FOUND 2 pillow ['Xception', 'VGG16']\n",
      "                                     FOUND 2 book_jacket ['VGG16', 'ResNet152']\n",
      "                                     FOUND 2 pajama ['MobileNetV2', 'ResNet50']\n",
      "                                     FOUND 2 comic_book ['ResNet50', 'ResNet152']\n",
      "                                     REST 4 12 ['brassiere', 'head_cabbage', 'face_powder', 'jersey', 'sock', 'cowboy_hat', 'artichoke']\n",
      "\n",
      "00039780dfc94d01db8676fe789ecd05.jpg 1\n",
      "\n",
      "collect   32412\n",
      "joind     32412\n",
      "file_tags 32412\n",
      "\n",
      "45153 Xception\n",
      "51692 VGG16\n",
      "51906 VGG19\n",
      "51987 DenseNet201\n",
      "44412 NASNetLarge\n",
      "41361 MobileNetV2\n",
      "51081 ResNet50\n",
      "50742 ResNet152\n",
      "42116 InceptionV3\n",
      "47479 IncResNetV2\n",
      "\n",
      "0 12\n",
      "1 0\n",
      "2 337\n",
      "3 1355\n",
      "4 2526\n",
      "5 3341\n",
      "6 3935\n",
      "7 3940\n",
      "8 4124\n",
      "9 4452\n",
      "10 8390\n",
      "\n",
      "0: 12\n",
      "2-5 : 7559\n",
      "5-10: 28182\n",
      "2-10: 32400\n",
      "write: D:/_DataScience/CapstoneProject/CnnGridSearch/newtags.pickle\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "newtags = dict()\n",
    "#joind, found, count = processing['apply']\n",
    "\n",
    "joind, found, count = sem.find_predictions(collect, get=5, show=0)\n",
    "all_tags, file_tags, tags_list, count_num, count_cnn = sem.tag_grouping(joind, booster=False, show=2)\n",
    "\n",
    "newtags['find'] = all_tags, file_tags, tags_list, count_num, count_cnn\n",
    "\n",
    "print()\n",
    "print('collect  ', len(collect))\n",
    "print('joind    ', len(joind))\n",
    "print('file_tags', len(file_tags))\n",
    "print()\n",
    "for cnn in count_cnn:\n",
    "    print(count_cnn[cnn], cnn)\n",
    "print()\n",
    "for num in count_num:\n",
    "    print(num, count_num[num])\n",
    "print()     \n",
    "print('0:',count_num[0])\n",
    "print('2-5 :',sum([count_num[x] for x in range(2,5+1)]))\n",
    "print('5-10:',sum([count_num[x] for x in range(5,len(models.keys())+1)]))\n",
    "print('2-10:',sum([count_num[x] for x in range(2,len(models.keys())+1)]))\n",
    "\n",
    "joind, found, count = sem.apply_threshold(collect, threshold=0.1, show=0)\n",
    "all_tags, file_tags, tags_list, count_num, count_cnn = sem.tag_grouping(joind, booster=False, show=2)\n",
    "\n",
    "newtags['apply'] = all_tags, file_tags, tags_list, count_num, count_cnn\n",
    "print()\n",
    "print('collect  ', len(collect))\n",
    "print('joind    ', len(joind))\n",
    "print('file_tags', len(file_tags))\n",
    "print()\n",
    "for cnn in count_cnn:\n",
    "    print(count_cnn[cnn], cnn)\n",
    "print()\n",
    "for num in count_num:\n",
    "    print(num, count_num[num])\n",
    "print()     \n",
    "print('0:',count_num[0])\n",
    "print('2-5 :',sum([count_num[x] for x in range(2,5+1)]))\n",
    "print('5-10:',sum([count_num[x] for x in range(5,len(models.keys())+1)]))\n",
    "print('2-10:',sum([count_num[x] for x in range(2,len(models.keys())+1)]))\n",
    "\n",
    "ct.write_dict('newtags',newtags,DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a20fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save/Load New-Tags Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99468cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "ct.write_dict('newtags',newtags,DATA) \n",
    "newtags = ct.read_dict('newtags'DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dde60582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['find', 'apply'])\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(newtags.keys())\n",
    "print(newtags['apply'][3].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62858e7a",
   "metadata": {},
   "source": [
    "### Print New-Tags aka CNN Grid-Sreach Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5ad1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000a68812bc7e98c42888dfb1c07da0.jpg (4, 12, ['book_jacket', 'comic_book', 'pajama', 'pillow', 'wool'])\n",
      "00039780dfc94d01db8676fe789ecd05.jpg (9, 18, ['brass', 'face_powder', 'puck'])\n",
      "000a190fdd715a2a36faed16e2c65df7.jpg (8, 16, ['face_powder', 'magnetic_compass', 'measuring_cup'])\n",
      "00117e4fc239b1b641ff08340b429633.jpg (9, 16, ['gown', 'pajama', 'sarong'])\n",
      "00136d1cf4edede0203f32f05f660588.jpg (7, 19, ['digital_watch', 'hair_spray', 'lotion', 'water_bottle'])\n",
      "0013e7355ffc5ff8fb1ccad3e42d92fe.jpg (4, 11, ['jean', 'miniskirt', 'poncho', 'stole'])\n",
      "00144a49c56599d45354a1c28104c039.jpg (5, 15, ['academic_gown', 'bearskin', 'drum', 'vestment'])\n",
      "0014f61389cbaa687a58e38a97b6383d.jpg (9, 22, ['cloak', 'gown', 'hoopskirt', 'overskirt'])\n",
      "0019a3c6755a194cb2e2c12bfc63972e.jpg (6, 15, ['bib', 'hair_slide', 'rubber_eraser', 'safety_pin'])\n",
      "001be52b2beec40ddc1d2d7fc7a68f08.jpg (9, 14, ['clog', 'loafer', 'sandal'])\n"
     ]
    }
   ],
   "source": [
    "show = 10\n",
    "for file in file_tags:\n",
    "    print(file,file_tags[file])\n",
    "    show -= 1\n",
    "    if show == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43703e",
   "metadata": {},
   "source": [
    "## Cross Analyse New Grid-Search Feature with Shopee Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5574f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0000a68812bc7e98c42888dfb1c07da0.jpg 5 train_129225211\n",
      "                                     FOUND 4 wool\n",
      "                                     FOUND 2 pillow\n",
      "                                     FOUND 2 book_jacket\n",
      "                                     FOUND 2 pajama\n",
      "                                     FOUND 2 comic_book\n",
      "                                     REST 12 ['brassiere', 'head_cabbage', 'face_powder', 'jersey', 'sock', 'cowboy_hat', 'artichoke']\n",
      "\n",
      "00039780dfc94d01db8676fe789ecd05.jpg 4 train_3386243561\n",
      "                                     FOUND 9 puck\n",
      "                                     FOUND 7 face_powder\n",
      "                                     FOUND 2 brass\n",
      "                                     REST 18 ['bottlecap', 'barrel']\n",
      "\n",
      "000a190fdd715a2a36faed16e2c65df7.jpg 3 train_2288590299\n",
      "                                     FOUND 8 face_powder\n",
      "                                     FOUND 6 measuring_cup\n",
      "                                     FOUND 2 magnetic_compass\n",
      "                                     REST 16 ['petri_dish', 'pill_bottle', 'tray', 'lotion']\n",
      "\n",
      "00117e4fc239b1b641ff08340b429633.jpg 2 train_2406599165\n",
      "                                     FOUND 9 pajama\n",
      "                                     FOUND 4 gown\n",
      "                                     FOUND 3 sarong\n",
      "                                     REST 16 ['apron', 'overskirt', 'ocarina']\n",
      "\n",
      "00136d1cf4edede0203f32f05f660588.jpg 1 train_3369186413\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classes_thr, classes_god, classes_all = sem.nlp_tag_grouping(joind, booster=False, show=5, threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9848bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34250\n",
      "['wool', 'wool', 'wool', 'wool']\n",
      "['wool', 'wool', 'wool', 'wool', 'pillow', 'pillow', 'book_jacket', 'book_jacket', 'pajama', 'pajama', 'comic_book', 'comic_book']\n",
      "['wool', 'wool', 'wool', 'wool', 'pillow', 'pillow', 'book_jacket', 'book_jacket', 'pajama', 'pajama', 'comic_book', 'comic_book', 'brassiere', 'head_cabbage', 'face_powder', 'jersey', 'sock', 'cowboy_hat', 'artichoke']\n"
     ]
    }
   ],
   "source": [
    "print(len(classes_thr))\n",
    "print(classes_thr['train_129225211'])\n",
    "print(classes_god['train_129225211'])\n",
    "print(classes_all['train_129225211'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009faf4",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814ea1b",
   "metadata": {},
   "source": [
    "This NLP solution is by the courtesy of Niels-Chrstian Leight and Elias BÃ¼chner from: NLP-Notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a59762",
   "metadata": {},
   "source": [
    "### Preprocessing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32da9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friendly borrowed by Nikhil Manali\n",
    "# https://www.kaggle.com/coder247/similarity-using-word2vec-text\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            if token == 'xxxx':\n",
    "                continue\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def word2vec_model(processed_docs, size_feat_vec=50):\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                         window=3,\n",
    "                         size=size_feat_vec,\n",
    "                         sample=6e-5, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         negative=20)\n",
    "    \n",
    "    w2v_model.build_vocab(processed_docs)\n",
    "    w2v_model.train(processed_docs, \n",
    "                    total_examples=w2v_model.corpus_count, \n",
    "                    epochs=300, \n",
    "                    report_delay=1)\n",
    "    \n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8285277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>preprocess_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[paper, victoria, secret]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>[doubl, tape, origin, doubl, foam, tape]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                          preprocess_title\n",
       "0   train_129225211                 [paper, victoria, secret]\n",
       "1  train_3386243561  [doubl, tape, origin, doubl, foam, tape]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_w2v = load_trainCsv()\n",
    "processed_docs = df_train_w2v['title'].map(preprocess)\n",
    "processed_docs = list(processed_docs)\n",
    "\n",
    "df_train_w2v['preprocess_title']=processed_docs\n",
    "df_train_w2v[['posting_id','preprocess_title']][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128e38c",
   "metadata": {},
   "source": [
    "### Creation of DataFrame() with CNNSemanticClustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5563f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34250\n",
      "['wool', 'wool', 'wool', 'wool']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_group</th>\n",
       "      <th>posting_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211</td>\n",
       "      <td>wool wool wool wool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>puck puck puck puck puck puck puck puck puck f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2395904891</td>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>face_powder face_powder face_powder face_powde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093212188</td>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>pajama pajama pajama pajama pajama pajama paja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3648931069</td>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>lotion lotion lotion lotion lotion lotion loti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_group        posting_id  \\\n",
       "0   249114794   train_129225211   \n",
       "1  2937985045  train_3386243561   \n",
       "2  2395904891  train_2288590299   \n",
       "3  4093212188  train_2406599165   \n",
       "4  3648931069  train_3369186413   \n",
       "\n",
       "                                               title  \n",
       "0                                wool wool wool wool  \n",
       "1  puck puck puck puck puck puck puck puck puck f...  \n",
       "2  face_powder face_powder face_powder face_powde...  \n",
       "3  pajama pajama pajama pajama pajama pajama paja...  \n",
       "4  lotion lotion lotion lotion lotion lotion loti...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dict()\n",
    "features = dict() \n",
    "df = pd.DataFrame()\n",
    "\n",
    "key = 'all' # 'thr'  'god'  'all'\n",
    "\n",
    "if key == 'thr':\n",
    "    classes = classes_thr\n",
    "elif key == 'god':\n",
    "    classes = classes_god\n",
    "else:\n",
    "    classes = classes_all\n",
    "\n",
    "print(len(classes_thr))\n",
    "print(classes_thr['train_129225211'])\n",
    "\n",
    "for pid in classes:\n",
    "    label_group = str(dict_begin['dic_posting_id_label_group'][pid][0])\n",
    "    features.update({'posting_id':pid})\n",
    "    features.update({'label_group':label_group})\n",
    "    features.update({'title':' '.join(classes_thr[pid])})\n",
    "    df = df.append(features, ignore_index=True)\n",
    "\n",
    "# convert dict() into pd.DataFrame()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb00168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>preprocess_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[wool, wool, wool, wool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>[puck, puck, puck, puck, puck, puck, puck, puc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>[face_powd, face_powd, face_powd, face_powd, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[pajama, pajama, pajama, pajama, pajama, pajam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>[lotion, lotion, lotion, lotion, lotion, lotio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                   preprocess_title\n",
       "0   train_129225211                           [wool, wool, wool, wool]\n",
       "1  train_3386243561  [puck, puck, puck, puck, puck, puck, puck, puc...\n",
       "2  train_2288590299  [face_powd, face_powd, face_powd, face_powd, f...\n",
       "3  train_2406599165  [pajama, pajama, pajama, pajama, pajama, pajam...\n",
       "4  train_3369186413  [lotion, lotion, lotion, lotion, lotion, lotio..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if not 'df_train_w2v' in locals():\n",
    "#    df_train_w2v = dict()\n",
    "\n",
    "df_train_w2v = dict()\n",
    "\n",
    "df_train_w2v[key] = df.copy()\n",
    "\n",
    "processed_docs = dict()\n",
    "processed_docs[key] = df_train_w2v[key]['title'].map(preprocess)\n",
    "processed_docs[key] = list(processed_docs[key])\n",
    "\n",
    "df_train_w2v[key]['preprocess_title']=processed_docs[key]\n",
    "df_train_w2v[key][['posting_id','preprocess_title']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b1e357a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['odomet',\n",
       " 'odomet',\n",
       " 'odomet',\n",
       " 'odomet',\n",
       " 'analog_clock',\n",
       " 'analog_clock',\n",
       " 'analog_clock',\n",
       " 'analog_clock']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[key][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a19ed0",
   "metadata": {},
   "source": [
    "### Building a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e921ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "build_new_model_bool = False\n",
    "\n",
    "processed = list(processed_docs[key])\n",
    "\n",
    "if build_new_model_bool:\n",
    "    w2v_model = word2vec_model(processed, size_feat_vec=50)\n",
    "    w2v_model.save(DATA+'word2vec_model_'+key+'.pickle')\n",
    "else:\n",
    "    w2v_model = pickle.load(open(DATA+'word2vec_model_'+key+'.pickle', 'rb'))\n",
    "\n",
    "if not 'emb_vec' in locals():\n",
    "    emb_vec = dict()\n",
    "emb_vec[key] = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f7c7f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\anaconda3\\envs\\tf_gpu_36\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34250, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_group</th>\n",
       "      <th>posting_id</th>\n",
       "      <th>title</th>\n",
       "      <th>preprocess_title</th>\n",
       "      <th>word_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211</td>\n",
       "      <td>wool wool wool wool</td>\n",
       "      <td>[wool, wool, wool, wool]</td>\n",
       "      <td>[0.006743095288388861, -0.004858696665403262, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>puck puck puck puck puck puck puck puck puck f...</td>\n",
       "      <td>[puck, puck, puck, puck, puck, puck, puck, puc...</td>\n",
       "      <td>[-0.20203430882502466, 0.006678147491851313, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_group        posting_id  \\\n",
       "0   249114794   train_129225211   \n",
       "1  2937985045  train_3386243561   \n",
       "\n",
       "                                               title  \\\n",
       "0                                wool wool wool wool   \n",
       "1  puck puck puck puck puck puck puck puck puck f...   \n",
       "\n",
       "                                    preprocess_title  \\\n",
       "0                           [wool, wool, wool, wool]   \n",
       "1  [puck, puck, puck, puck, puck, puck, puck, puc...   \n",
       "\n",
       "                                            word_vec  \n",
       "0  [0.006743095288388861, -0.004858696665403262, ...  \n",
       "1  [-0.20203430882502466, 0.006678147491851313, -...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature_vec_v2(sen1, model, size_feat_vec=50):\n",
    "    \n",
    "    sen_vec1 = np.zeros(size_feat_vec)\n",
    "    for val in sen1:\n",
    "        sen_vec1 = np.add(sen_vec1, model[val])    \n",
    "    return sen_vec1/norm(sen_vec1)\n",
    "\n",
    "df_train_w2v[key]['word_vec'] = df_train_w2v[key].apply(\n",
    "    lambda row: get_feature_vec_v2(row['preprocess_title'], emb_vec[key]), axis=1)\n",
    "\n",
    "print(df_train_w2v[key].shape)\n",
    "df_train_w2v[key].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a966ea",
   "metadata": {},
   "source": [
    "### Calculate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a83c662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>1</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_598686012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>1</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_578257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22962</th>\n",
       "      <td>0.994298</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_395732057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21920</th>\n",
       "      <td>0.944525</td>\n",
       "      <td>3185007248</td>\n",
       "      <td>train_574223332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25472</th>\n",
       "      <td>0.937145</td>\n",
       "      <td>452508504</td>\n",
       "      <td>train_3343934633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>0.937145</td>\n",
       "      <td>4288078681</td>\n",
       "      <td>train_635337083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>0.937139</td>\n",
       "      <td>3185007248</td>\n",
       "      <td>train_4082031443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14260</th>\n",
       "      <td>0.935965</td>\n",
       "      <td>1790344296</td>\n",
       "      <td>train_657274189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>0.935072</td>\n",
       "      <td>912146474</td>\n",
       "      <td>train_2710843880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26938</th>\n",
       "      <td>0.935072</td>\n",
       "      <td>2393818132</td>\n",
       "      <td>train_3813210970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1                 2\n",
       "8400          1   912146474   train_598686012\n",
       "17008         1   912146474   train_578257500\n",
       "22962  0.994298   912146474   train_395732057\n",
       "21920  0.944525  3185007248   train_574223332\n",
       "25472  0.937145   452508504  train_3343934633\n",
       "5006   0.937145  4288078681   train_635337083\n",
       "28614  0.937139  3185007248  train_4082031443\n",
       "14260  0.935965  1790344296   train_657274189\n",
       "3326   0.935072   912146474  train_2710843880\n",
       "26938  0.935072  2393818132  train_3813210970"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_1=8400\n",
    "i_vec_1   = df_train_w2v[key]['word_vec'][id_1]\n",
    "i_vec_all = df_train_w2v[key]['word_vec'].values\n",
    "i_vec_all = np.vstack(i_vec_all)\n",
    "\n",
    "labels     = df_train_w2v[key]['label_group'].to_list()\n",
    "posting_id = df_train_w2v[key]['posting_id'].to_list()\n",
    "list1      = list(sc.get_sim_all_pi(i_vec_1,i_vec_all))\n",
    "\n",
    "if not 'df_nlp' in locals():\n",
    "    df_nlp = dict()\n",
    "df_nlp[key] = pd.DataFrame(data=[list1,labels,posting_id]).transpose()\n",
    "df_nlp[key] = df_nlp[key].sort_values(by=[0,1],ascending=False)\n",
    "df_nlp[key].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bf91b",
   "metadata": {},
   "source": [
    "### Create clusters optimized on recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c74f7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall:  0.5294256514727107   Mean Length of cluster:  188.68\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rec_values = []\n",
    "cl_size = []\n",
    "\n",
    "key = 'all' # 'thr'  'god'  'all'\n",
    "\n",
    "threshold = 0.98\n",
    "\n",
    "#for i in list(np.random.randint(34250, size=50)): <- ... does not work here\n",
    "for i in range(100):\n",
    "    clreal = sc.real_cluster_of_i_w2v(i*100,df_train_w2v[key])\n",
    "    clpred = sc.pred_cluster_of_i_w2v(i*100,threshold,df_train_w2v[key],labels,posting_id)\n",
    "    rec_values.append(recall_i(clreal,clpred))\n",
    "    cl_size.append(len(clpred))\n",
    "    \n",
    "print(\"Mean Recall: \",sum(rec_values)/len(rec_values), \"  Mean Length of cluster: \", sum(cl_size)/len(cl_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b60fc3",
   "metadata": {},
   "source": [
    "### Cluster creation for all-2-all (34250x34250): threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "adaf1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "Wall time: 8h 11min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "already_done = True\n",
    "already_done = False\n",
    "\n",
    "threshold = 0.99\n",
    "DO = 34250\n",
    "DO = 100\n",
    "key = 'all' # 'thr'  'god'  'all'\n",
    "\n",
    "labels     = df_train_w2v[key]['label_group'].to_list()\n",
    "posting_id = df_train_w2v[key]['posting_id'].to_list()\n",
    "list1      = list(sc.get_sim_all_pi(i_vec_1,i_vec_all))\n",
    "\n",
    "if already_done == False:\n",
    "\n",
    "    dict_nlp_prec_all_97 = {}\n",
    "    list_post_id = df_train_w2v[key]['posting_id'].tolist()\n",
    "   \n",
    "    for i in range(25501,34250):    \n",
    "        dict_nlp_prec_all_97[list_post_id[i]] = set(sc.pred_cluster_of_i_w2v(i,threshold,df_train_w2v[key],labels,posting_id))\n",
    "        if i%1000 == 0:    # Display progress and save \n",
    "            print(i)\n",
    "            pickle.dump(dict_nlp_prec_all_97, open( DATA+\"dict_nlp_prec_all_97_run1.pickle\", \"wb\" ) )\n",
    "    pickle.dump(dict_nlp_prec_all_97, open( DATA+\"dict_nlp_prec_all_97_run1.pickle\", \"wb\" ) )    # Final save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574b7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "dict_nlp_prec_all_97_load = pickle.load( open( DATA+\"dict_nlp_prec_all_97_run1.pickle\", \"rb\" ) )\n",
    "temp = pickle.load( open( DATA+\"dict_nlp_prec_all_97_run2.pickle\", \"rb\" ) )\n",
    "dict_nlp_prec_all_97_load.update(temp)\n",
    "temp = pickle.load( open( DATA+\"dict_nlp_prec_all_97_run3.pickle\", \"rb\" ) )\n",
    "dict_nlp_prec_all_97_load.update(temp)\n",
    "temp = pickle.load( open( DATA+\"dict_nlp_prec_all_97_run4.pickle\", \"rb\" ) )\n",
    "dict_nlp_prec_all_97_load.update(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751da550",
   "metadata": {},
   "source": [
    "### Calculate the f1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2662cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine two predictions\n",
    "def combi_pred(pred1, pred2):\n",
    "\n",
    "    combi_set = {} \n",
    "    for i in pred1.keys():\n",
    "        assert i in set(pred2.keys())\n",
    "        combi_set[i] = pred1[i].union(pred2[i])\n",
    "    return combi_set\n",
    "\n",
    "# Load results from the pHash Notebook\n",
    "dict_phash_prec_all_9_load = pickle.load( open( DATA+\"dict_phash_prec_all_9.pickle\", \"rb\" ) )\n",
    "\n",
    "pred_nlp_phash = combi_pred(dict_nlp_prec_all_97_load, dict_phash_prec_all_9_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0116939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.28776551732169997\n",
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate F1-Score of the combination NLP and pHash\n",
    "\n",
    "fscores = []\n",
    "\n",
    "for i in range(1000,1100):\n",
    "    x = sc.f_score_i(sc.real_cluster_of_i_w2v(i,df_train_w2v[key]), pred_nlp_phash[df_train_w2v[key].posting_id.values[i]])\n",
    "    fscores.append(x)\n",
    "    \n",
    "print(\"F1-Score: \", sum(fscores)/len(fscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ba16a",
   "metadata": {},
   "source": [
    "### Calculate the f1-Score (with pHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b4a9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nlp_phash = combi_pred(dict_nlp_prec_all_97_load, dict_nlp_prec_all_97_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "408f373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.31655435955308664\n",
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate F1-Score of the combination NLP and pHash\n",
    "\n",
    "fscores = []\n",
    "\n",
    "for i in range(1000,1100):\n",
    "    x = sc.f_score_i(sc.real_cluster_of_i_w2v(i,df_train_w2v[key]), pred_nlp_phash[df_train_w2v[key].posting_id.values[i]])\n",
    "    fscores.append(x)\n",
    "    \n",
    "print(\"F1-Score: \", sum(fscores)/len(fscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3ed17",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e8152",
   "metadata": {},
   "source": [
    "The overall **f1-Score** of **0.288** is not as good as expected. Together with the **pHash** method the **f1-score** reaches **0.317**.\n",
    "\n",
    "With this result the method is not suited for prediction and cluster creation in the **Shopee** data set. The NLP algorithm has been used as is, it may not be setup up right for CNNSemanticClustering data - see **Future Work**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2746c31",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2600b3f",
   "metadata": {},
   "source": [
    "Due to time issues the hyper parameters for Word2Vec where not optimized. The **f1-Score** could potentially be improved by hyper parameter tuning.\n",
    "\n",
    "Though experience shows that often the improvement by hyper parameter tuning is limited.\n",
    "\n",
    "Methods like **Caption Prediction** or **Self-Trained CNN** seems more promising ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
