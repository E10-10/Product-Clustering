{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "In this notebook, we will build a first model based on VGG16, a model pre-trained on ImageNet. We'll pass the visual information representing the images in our data set through the convolutional layers of this network in order to generate flattened feature vectors out of all images. Given these vectors we can then define metrics to measure 'distances' between two pictures. This finally enables us to apply several clustering algorithms on our image set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os, os.path\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .csv file\n",
    "def load_trainCsv():\n",
    "    df = pd.read_csv('./input/train.csv')    \n",
    "    return df\n",
    "\n",
    "df_train = load_trainCsv()\n",
    "df_train = df_train.sort_values(by=\"label_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>train_1646767365</td>\n",
       "      <td>1d7aadc7503b2b4539cc9a5fe41979dd.jpg</td>\n",
       "      <td>e925873ed09cd08f</td>\n",
       "      <td>Sarung celana wadimor original 100% dewasa dan...</td>\n",
       "      <td>258047</td>\n",
       "      <td>train_1646767365 train_1528423085 train_398181303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31859</th>\n",
       "      <td>train_1528423085</td>\n",
       "      <td>eec692257e74fcbc6cb63cb76d0f20e7.jpg</td>\n",
       "      <td>ea97861c926a71e3</td>\n",
       "      <td>WARNA RANDOM ACAK Sarung Celana Wadimor MURAH ...</td>\n",
       "      <td>258047</td>\n",
       "      <td>train_1646767365 train_1528423085 train_398181303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>train_398181303</td>\n",
       "      <td>3301b8aaccea93d1098995ffbc537335.jpg</td>\n",
       "      <td>e9b5833e929e909c</td>\n",
       "      <td>SARUNG CELANA WADIMOR DEWASA HITAM POLOS SARCEL</td>\n",
       "      <td>258047</td>\n",
       "      <td>train_1646767365 train_1528423085 train_398181303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image  \\\n",
       "3874   train_1646767365  1d7aadc7503b2b4539cc9a5fe41979dd.jpg   \n",
       "31859  train_1528423085  eec692257e74fcbc6cb63cb76d0f20e7.jpg   \n",
       "6738    train_398181303  3301b8aaccea93d1098995ffbc537335.jpg   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "3874   e925873ed09cd08f  Sarung celana wadimor original 100% dewasa dan...   \n",
       "31859  ea97861c926a71e3  WARNA RANDOM ACAK Sarung Celana Wadimor MURAH ...   \n",
       "6738   e9b5833e929e909c    SARUNG CELANA WADIMOR DEWASA HITAM POLOS SARCEL   \n",
       "\n",
       "       label_group                                       matches_true  \n",
       "3874        258047  train_1646767365 train_1528423085 train_398181303  \n",
       "31859       258047  train_1646767365 train_1528423085 train_398181303  \n",
       "6738        258047  train_1646767365 train_1528423085 train_398181303  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column: matches_true\n",
    "tmp = df_train.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "df_train['matches_true'] = df_train['label_group'].map(tmp)\n",
    "df_train['matches_true'] = df_train['matches_true'].apply(lambda x: ' '.join(x))\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and normalizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and resize them to 224 x 224. This is the size our pre-trained model prefers.\n",
    "\n",
    "def load_images(df_train):\n",
    "    \n",
    "    folder_name = \"./input/shopee-product-matching/train_images\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    posting_id = []\n",
    "    \n",
    "    for p_id in df_train['posting_id']:       \n",
    "        img_name = df_train[df_train['posting_id'] == p_id]['image'].values[0]\n",
    "        img = folder_name + '/' + img_name\n",
    "    \n",
    "        if img[-4:] == \".jpg\":\n",
    "            image = cv2.imread(img)\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            # Save this visual information in one list (images), save the respective labels in another list (labels) \n",
    "            # The same for the posting_ids\n",
    "            images.append(image)\n",
    "            label = df_train[df_train[\"posting_id\"] == p_id][\"label_group\"].values[0]\n",
    "            labels.append(label)\n",
    "            posting_id.append(p_id)\n",
    "            \n",
    "    return images,labels, posting_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels,posting_id = load_images(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform the lists containing the visual information respectively the labels into NumPy arrays. Then we normalize the array containing the visual information. This will facilitate preprocessing them with the VGG16 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_images(images, labels):\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels)\n",
    "    # normalize the images array by dividing every component through 255\n",
    "    images /= 255    \n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = normalise_images(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16 model and transform images into feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the entire VGG16 model (including the fully connected layers). To get a quick overview on what the classifications from VGG16 look like, let's have a look on the first ten images out of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16(include_top=True, weights=\"imagenet\", input_shape=(224,224,3))\n",
    "p = vgg16_model.predict(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posting_id:  train_1646767365   label group:  258047\n",
      "posting_id:  train_1528423085   label group:  258047\n",
      "posting_id:  train_398181303   label group:  258047\n",
      "posting_id:  train_2865605743   label group:  297977\n",
      "posting_id:  train_1382500866   label group:  297977\n",
      "posting_id:  train_2070644662   label group:  645628\n",
      "posting_id:  train_2149563017   label group:  645628\n",
      "posting_id:  train_2419208039   label group:  645628\n",
      "posting_id:  train_318767180   label group:  645628\n",
      "posting_id:  train_789743463   label group:  645628\n"
     ]
    }
   ],
   "source": [
    "# print out the label groups the first ten images belong to\n",
    "for i in range(0,10):\n",
    "    print('posting_id: ', posting_id[i],   '  label group: ', labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n03788365', 'mosquito_net', 0.10819273),\n",
       "  ('n15075141', 'toilet_tissue', 0.07427242)],\n",
       " [('n03788365', 'mosquito_net', 0.14711374),\n",
       "  ('n15075141', 'toilet_tissue', 0.06362324)],\n",
       " [('n03788365', 'mosquito_net', 0.13091399),\n",
       "  ('n15075141', 'toilet_tissue', 0.06639648)],\n",
       " [('n03291819', 'envelope', 0.056172915),\n",
       "  ('n03788365', 'mosquito_net', 0.04525588)],\n",
       " [('n03788365', 'mosquito_net', 0.10130049),\n",
       "  ('n04209239', 'shower_curtain', 0.058161855)],\n",
       " [('n03291819', 'envelope', 0.11349606),\n",
       "  ('n07248320', 'book_jacket', 0.040203404)],\n",
       " [('n03788365', 'mosquito_net', 0.104926676),\n",
       "  ('n03291819', 'envelope', 0.07137099)],\n",
       " [('n03788365', 'mosquito_net', 0.1448797),\n",
       "  ('n15075141', 'toilet_tissue', 0.0840735)],\n",
       " [('n03788365', 'mosquito_net', 0.11199081),\n",
       "  ('n15075141', 'toilet_tissue', 0.06449302)],\n",
       " [('n03788365', 'mosquito_net', 0.09336743),\n",
       "  ('n15075141', 'toilet_tissue', 0.06524198)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the prediction VGG16 makes for these images\n",
    "decode_predictions(p[0:10][:],top = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the images have been classified as mosquito net. We also see that the probabilities with which our images belong to the respective clusters are quite low. This is not surprising, as this net was trained on completely different images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already explained above, we do not want to use VGG16 in order to make classifications on our data set, but to transform images into feature vectors on which we want to apply clustering methods. So the visual information only needs to go through the convolutional layers of VGG16. So what we need to do is to load this net again without the fully connected layers (set include_tod=False for this purpose) and make a 'prediction' which then will be flattened. This way we obtain the feature vectors we desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model from keras. Set include_top=False, as we want to build our model on the flattened output\n",
    "# of the CNN \n",
    "vgg16_model = keras.applications.vgg16.VGG16(include_top=False, weights=\"imagenet\", input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature vectore has 25088 entries\n"
     ]
    }
   ],
   "source": [
    "def covnet_transform(covnet_model, raw_images):\n",
    "    pred = covnet_model.predict(raw_images)\n",
    "    # Flatten the output\n",
    "    flat = pred.reshape(raw_images.shape[0], -1)    \n",
    "    return flat\n",
    "\n",
    "# transforming the images contained in X_train into feature vectors\n",
    "vgg16_output = covnet_transform(vgg16_model, X_train)\n",
    "\n",
    "# display the length of these vectors\n",
    "print(\"The feature vectore has {} entries\".format(vgg16_output.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction via PCA\n",
    "\n",
    "Even though the size of the feature vectors we just created is small compared to the information contained in the original matrices representing the picture (224 x 224 x 3), these vectors are still quite large and would make predictions computationally expensive. In order to decrease computation time, we now apply a principle component analysis on our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_PCA(data, n_components=100):\n",
    "    \n",
    "    p = PCA(n_components=n_components, random_state=42)\n",
    "    p.fit(data)    \n",
    "    return p\n",
    "    \n",
    "vgg16_pca = create_fit_PCA(vgg16_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a plot that displays the variance which is explained by the reduced amount of components\n",
    "def pca_cumsum_plot(pca):\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Explained variance')\n",
    "    plt.show()\n",
    "    \n",
    "pca_cumsum_plot(vgg16_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply PCA on the output vectores of our VGG16 model\n",
    "vgg16_output_pca = vgg16_pca.transform(vgg16_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary which containes the feature vectors (after PCA) to every posting_id\n",
    "dic_fv = dict(zip(df_train.posting_id.values,list(vgg16_output_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Based on the feature vectors we've just created via PCA, we now want to apply three clustering algorithms and check out to which extent they are able to find the label groups defined in our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means\n",
    "\n",
    "We'll first use a k-Means clustering. When evaluating the results, please be aware that we told the model that the number of clusters should be the actual number of label groups contained in our data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_kmeans(data, number_of_clusters = len(set(y_train))):       \n",
    "    k = KMeans(n_clusters=number_of_clusters, n_jobs = -1, random_state=42)\n",
    "    k.fit(data)    \n",
    "    return k\n",
    "\n",
    "K_vgg16_pca = create_train_kmeans(vgg16_output_pca)\n",
    "k_vgg16_pred_pca = K_vgg16_pca.predict(vgg16_output_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns in the DataFrame\n",
    "df_train[\"kmeans_cluster\"] = np.array(k_vgg16_pred_pca)\n",
    "tmp = df_train.groupby(['kmeans_cluster'])['posting_id'].unique().to_dict()\n",
    "df_train['matches_pred'] = df_train['kmeans_cluster'].map(tmp)\n",
    "df_train['matches_pred'] = df_train['matches_pred'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the F1-Score of this prediction made with k-Means\n",
    "def f1_score_kmeans(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"f1_kmeans\"] = f1_score_kmeans(df_train.matches_true,df_train.matches_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score k-Means:     0.5050584059465884\n"
     ]
    }
   ],
   "source": [
    "print('F1-Score k-Means:    ',df_train.f1_kmeans.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering via Euclidean distance\n",
    "\n",
    "In this section, we'll define clusters by measuring the Euclidean distance between all pairs of images (which are now represented by feature vectors). For that purpose we'll simply define a threshold to determine how close two images have to be in order to be considered as belonging to the same cluster. Then we determine for all pictures separately those neighbours which lie within that threshold. This way we obtain a predicted cluster for every single picture. Unlike the clusters we get from k-Means or from DBSCAN, these clusters are not necessarily disjoint (very probably they are not). This is no problem at all as our target metric, the F1-Score, expects predicted clusters for every image which is then compared to the real cluster this image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method calculates the Euclidean distance between one feature vector and all the others\n",
    "def calc_distance(dic,pid):\n",
    "    s1 = dic[pid]\n",
    "    dist = list()\n",
    "    for k in dic.keys():\n",
    "        s2 = dic[k] \n",
    "        d = np.sqrt(np.sum(np.square(s1 - s2)))\n",
    "        dist.append(d)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>label_group</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_3645016213</td>\n",
       "      <td>645628</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17389</th>\n",
       "      <td>train_38309879</td>\n",
       "      <td>2152124620</td>\n",
       "      <td>23.705448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16866</th>\n",
       "      <td>train_2649294003</td>\n",
       "      <td>2092158825</td>\n",
       "      <td>23.994318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13611</th>\n",
       "      <td>train_1471123207</td>\n",
       "      <td>1687595158</td>\n",
       "      <td>24.930891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>train_3989743853</td>\n",
       "      <td>721559761</td>\n",
       "      <td>24.948362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10157</th>\n",
       "      <td>train_4053748246</td>\n",
       "      <td>1226243819</td>\n",
       "      <td>54.435429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15254</th>\n",
       "      <td>train_3691504993</td>\n",
       "      <td>1891360161</td>\n",
       "      <td>54.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10152</th>\n",
       "      <td>train_2998702168</td>\n",
       "      <td>1226243819</td>\n",
       "      <td>55.270214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7618</th>\n",
       "      <td>train_4122448751</td>\n",
       "      <td>947423206</td>\n",
       "      <td>55.523033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>train_4172891622</td>\n",
       "      <td>1226243819</td>\n",
       "      <td>55.668423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id  label_group   distance\n",
       "10     train_3645016213       645628   0.000000\n",
       "17389    train_38309879   2152124620  23.705448\n",
       "16866  train_2649294003   2092158825  23.994318\n",
       "13611  train_1471123207   1687595158  24.930891\n",
       "5750   train_3989743853    721559761  24.948362\n",
       "...                 ...          ...        ...\n",
       "10157  train_4053748246   1226243819  54.435429\n",
       "15254  train_3691504993   1891360161  54.993553\n",
       "10152  train_2998702168   1226243819  55.270214\n",
       "7618   train_4122448751    947423206  55.523033\n",
       "10153  train_4172891622   1226243819  55.668423\n",
       "\n",
       "[34250 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the nearest neighbours of one particular image\n",
    "def check_one_pid(dic_fv,df, pid = \"train_3645016213\"):\n",
    "    l = calc_distance(dic_fv, pid)\n",
    "    l[:10]\n",
    "    dist_df = pd.DataFrame({\"posting_id\":df.posting_id.values,\n",
    "                            \"label_group\": df.label_group.values,\n",
    "                            \"distance\": l,                            \n",
    "                           })\n",
    "    dist_df = dist_df.sort_values(by = \"distance\")\n",
    "    return dist_df\n",
    "\n",
    "check_one_pid(dic_fv,df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions in order to do the evaluation\n",
    "def pred_cluster_of_pid_cnn(pid, dic_fv, threshold):\n",
    "    fv1 = dic_fv[pid]\n",
    "    list_pid = list(dic_fv.keys())\n",
    "    dist = calc_distance(dic_fv,pid)\n",
    "    df_out = pd.DataFrame({\"posting_id\":list_pid, \"distance\":dist})\n",
    "    df_out = df_out[df_out[\"distance\"] <= threshold]\n",
    "    ls = df_out[\"posting_id\"].tolist()\n",
    "    return set(ls)\n",
    "\n",
    "def real_cluster_of_pid(pid, df):\n",
    "    df_red_list = df[df[\"posting_id\"] == pid][\"matches_true\"].apply(lambda x: x.split()).values[0]\n",
    "    return set(df_red_list)\n",
    "\n",
    "def f_score(cl_real_i, cl_pred_i):\n",
    "    s_pred = set(cl_pred_i)\n",
    "    s_real = set(cl_real_i)\n",
    "    s_intsec = s_pred.intersection(s_real)\n",
    "    return 2*len(s_intsec) / (len(s_pred)+len(s_real))\n",
    "\n",
    "def eval_f1(l_th, df, dic_fv):\n",
    "    f1 = [[] for _ in range(len(l_th))]\n",
    "    for i, th in enumerate(l_th):\n",
    "        for pi in df.posting_id:\n",
    "            f1[i].append(f_score(real_cluster_of_pid(pi, df),pred_cluster_of_pid_cnn(pi,dic_fv,th)))\n",
    "        print(\"Threshold : {}      F1-Score: {}\".format(th, np.sum(f1[i]) / len(f1[i])))\n",
    "    return f1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold : 24      F1-Score: 0.381011314778529\n"
     ]
    }
   ],
   "source": [
    "f1 = eval_f1([24],df_train,dic_fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with DBSCAN\n",
    "\n",
    "Finally we'll do a clustering using the DBSCAN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scan_model = DBSCAN(\n",
    "                    eps=0.4,\n",
    "                    min_samples=2,\n",
    "                    metric='euclidean',\n",
    "                    metric_params=None,\n",
    "                    algorithm='auto',\n",
    "                    leaf_size=30,\n",
    "                    p=None,\n",
    "                    n_jobs=None,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:    1618\n"
     ]
    }
   ],
   "source": [
    "db_pred = db_scan_model.fit_predict(list(dic_fv.values()))\n",
    "print('Number of clusters:   ',len(set(db_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"db_scan\"] = db_pred\n",
    "tmp = df_train.groupby(['db_scan'])['posting_id'].unique().to_dict()\n",
    "if -1 in tmp.keys(): \n",
    "    del tmp[-1]\n",
    "df_train[\"db_scan\"] = [x if df_train[df_train[\"posting_id\"] == x][\"db_scan\"].values[0] == -1 else tmp[df_train[df_train[\"posting_id\"] == x][\"db_scan\"].values[0]] for x in df_train.posting_id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.split()\n",
    "    intersection = np.intersect1d(y_true,y_pred)\n",
    "    intersection = len(intersection)\n",
    "    len_y_pred = len(y_pred)\n",
    "    len_y_true = len(y_true)\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_db_scan = []\n",
    "for y_true, y_pred in zip(df_train.matches_true.values, df_train.db_scan.values):\n",
    "    f1_db_scan.append(f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score DBSCAN:    0.1618436010315891\n"
     ]
    }
   ],
   "source": [
    "# Print out the final F1-Score\n",
    "print('F1-Score DBSCAN:   ',np.sum(f1_db_scan)/len(f1_db_scan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the F1-Score we got from the k-Means clustering was not too bad, but still there is plenty of room for improvement. As the VGG16 model was trained on images which are quite different from those contained in our data set, we did not expect perfect results anyway. Future work on this approach could be a transfer learning model: Instead of simply clustering the feature vectors we got from the convolutional layers of the VGG16 model, we could use these vectors as the input data for a new neural network. This network would then be trained to recognize the specific objects displayed on the images contained in our data set. As the feature vectors we would get from such a method would be much better adapted to the objects we want to cluster, this gives cause to hope to achieve better results that way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
